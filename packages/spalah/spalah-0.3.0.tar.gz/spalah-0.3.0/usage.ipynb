{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spalah.dataframe import slice_dataframe, script_dataframe, flatten_schema\n",
    "from pyspark.sql import SparkSession, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),    \n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "a\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spalah.dataframe import slice_dataframe\n",
    "\n",
    "slice_dataframe(\n",
    "    input_dataframe=df,    \n",
    "    \n",
    "    nullify_only=False\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_df = spark.sql(\"\"\"\n",
    "select \n",
    "    1 as column_a\n",
    ",   2.0 as column_b\n",
    ",   struct(\n",
    "        \"c1\" as column_c_1\n",
    "    ,   struct(\n",
    "            \"c_2_1\" as c_2_1,\n",
    "            \"c_2_2\" as c_2_2,\n",
    "            \"c_2_3\" as c_2_3\n",
    "    ) as column_c_2\n",
    ") as column_c\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_dataframe(\n",
    "    input_dataframe=nested_df,\n",
    "    columns_to_include=[\"column_c\"],\n",
    "    columns_to_exclude=[\"column_c.column_c_1\"],\n",
    "    nullify_only=False,\n",
    "    debug=False\n",
    ").select(\"column_c.*\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+---+\n",
      "|  a|  b|      c|         d|                  e|  q|\n",
      "+---+---+-------+----------+-------------------+---+\n",
      "|  1| 2.|string1|2000-01-01|2000-01-01 12:00:00|bcd|\n",
      "+---+---+-------+----------+-------------------+---+\n",
      "\n",
      "+---+---+-------+----------+-------------------+---+\n",
      "|  A|  b|      c|         d|                  e|  f|\n",
      "+---+---+-------+----------+-------------------+---+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|abc|\n",
      "+---+---+-------+----------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    Row(a=1, b=\"2.\", c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0), q=\"bcd\"),    \n",
    "])\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    Row(A=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0), f=\"abc\"),    \n",
    "])\n",
    "df2.show()\n",
    "\n",
    "schema1 = flatten_schema(df1.schema, True)\n",
    "schema2 = flatten_schema(df2.schema, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('e', 'TimestampType'), ('c', 'StringType'), ('d', 'DateType')}\n"
     ]
    }
   ],
   "source": [
    "_source = set(schema1.copy())\n",
    "_target = set(schema2.copy())\n",
    "\n",
    "get_matched_by_name_and_type = lambda source, target: source & target\n",
    "\n",
    "# Case 1: Find all matched columns, keep them in separate list\n",
    "list_matched = get_matched_by_name_and_type(_source, _target)\n",
    "\n",
    "print(list_matched)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('b', 'StringType'), ('a', 'LongType'), ('q', 'StringType')}\n",
      "{('f', 'StringType'), ('b', 'DoubleType'), ('A', 'LongType')}\n"
     ]
    }
   ],
   "source": [
    "remove_by_name_and_type = lambda base_value, subtract_value: base_value - subtract_value\n",
    "\n",
    "_source = remove_by_name_and_type(_source, list_matched)\n",
    "_target = remove_by_name_and_type(_target, list_matched)\n",
    "\n",
    "print(_source)\n",
    "print(_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('a', 'LongType')}\n",
      "[('b', 'StringType'), ('q', 'StringType')]\n",
      "[('f', 'StringType'), ('b', 'DoubleType')]\n"
     ]
    }
   ],
   "source": [
    "column_names_lower = lambda base_value: set([(x.lower(), y) for (x, y) in base_value])\n",
    "remove_by_name = lambda base_value, subtract_value: [(x, y) for (x, y) in base_value if not x.lower() in ([z[0] for z in subtract_value])]\n",
    "\n",
    "_source_lowered = column_names_lower(_source)\n",
    "_target_lowered =  column_names_lower(_target)\n",
    "\n",
    "list_not_matched_by_case = get_matched_by_name_and_type(_source_lowered, _target_lowered)\n",
    "\n",
    "print(list_not_matched_by_case)\n",
    "\n",
    "_source = remove_by_name(_source, list_not_matched_by_case)\n",
    "_target = remove_by_name(_target, list_not_matched_by_case)\n",
    "\n",
    "print(_source)\n",
    "print(_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_matched_by_name_but_not_type(source, target): \n",
    "    x = dict(source)\n",
    "    y = dict(target)\n",
    "    return [(k, f\"{x[k]} <=> {y[k]}\") for k in x if k in y and x[k] != y[k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 'StringType <=> DoubleType')]\n"
     ]
    }
   ],
   "source": [
    "list_not_matched_by_type = get_matched_by_name_but_not_type(_source, _target)\n",
    "print(list_not_matched_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('q', 'StringType')]\n",
      "[('f', 'StringType')]\n"
     ]
    }
   ],
   "source": [
    "_source =  remove_by_name(_source, list_not_matched_by_type)\n",
    "_target =  remove_by_name(_target, list_not_matched_by_type)\n",
    "\n",
    "print(_source)\n",
    "print(_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists only the source:\n",
      "[('q', 'StringType')]\n"
     ]
    }
   ],
   "source": [
    "if _source:\n",
    "    print(\"exists only the source:\")\n",
    "    print(_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists only the target:\n",
      "[('f', 'StringType')]\n"
     ]
    }
   ],
   "source": [
    "if _target:\n",
    "    print(\"exists only the target:\")\n",
    "    print(_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from pyspark.sql import types as T\n",
    "from typing import Set\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "MatchedColumn = namedtuple('MatchedColumn', ['name', 'data_type'])\n",
    "NotMatchedColumn = namedtuple('NotMatchedColumn', ['name', 'data_type', 'reason'])\n",
    "\n",
    "class  SchemaComparer():\n",
    "\n",
    "    def __init__(self, source_schema: T.StringType, target_schema: T.StringType) -> None:\n",
    "        self._source = self.__import_schema(source_schema)\n",
    "        self._target = self.__import_schema(target_schema)\n",
    "        self.matched = list()\n",
    "        self.not_matched = list()\n",
    "\n",
    "\n",
    "    def __import_schema(self, input_schema: T.StructType) -> Set[tuple]:\n",
    "        \"\"\"Import StructType as the flatten set of tuples: (column_name, data_type)\n",
    "\n",
    "        Args:\n",
    "            input_schema (T.StructType): Schema to process\n",
    "\n",
    "        Raises:\n",
    "            TypeError: if input schema has a type: DataFrame\n",
    "            TypeError: if input schema hasn't a type: StructType\n",
    "\n",
    "        Returns:\n",
    "            Set[tuple]: Set of tuples: (column_name, data_type)\n",
    "        \"\"\"        \n",
    "\n",
    "        if isinstance(input_schema, DataFrame):\n",
    "            raise TypeError(\"One of 'source_schema or 'target_schema' passed as a DataFrame. Use DataFrame.schema instead\")\n",
    "        elif not isinstance(input_schema, T.StructType):\n",
    "            raise TypeError(\"Parameters 'source_schema and 'target_schema' must have a type: StructType\")\n",
    "       \n",
    "        return set(flatten_schema(input_schema, True))\n",
    "\n",
    "\n",
    "    def __match_by_name_and_type(\n",
    "        self, \n",
    "        source: Set[tuple] = set(), \n",
    "        target: Set[tuple] = set()\n",
    "    ) -> Set[tuple]:\n",
    "        \"\"\"Matches columns in source and target schemas by name and data type\n",
    "\n",
    "        Args:\n",
    "            source (Set[tuple], optional): Flattened source schema. Defaults to set().\n",
    "            target (Set[tuple], optional): Flattened target schema. Defaults to set().\n",
    "\n",
    "        Returns:\n",
    "            Set[tuple]: Fully matched columns as a set of tuples: (column_name, data_type)\n",
    "        \"\"\"        \n",
    "\n",
    "        # If source and target is not provided, use class attributes as the input        \n",
    "        _source = self._source if not source else source\n",
    "        _target = self._target if not target else target\n",
    "\n",
    "\n",
    "        result = _source & _target\n",
    "\n",
    "         # Remove matched values of case 1 from further processing\n",
    "        self.__remove_matched_by_name_and_type(result)\n",
    "        \n",
    "        if not (source and target):\n",
    "            self.__populate_matched(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __remove_matched_by_name_and_type(self, subtract_value: Set[tuple]) -> None: \n",
    "        \"\"\"Removes fully matched columns from the further processing\n",
    "\n",
    "        Args:\n",
    "            subtract_value (Set[tuple]): Set of matched columns\n",
    "        \"\"\"        \n",
    "        \n",
    "        self._source = self._source - subtract_value\n",
    "        self._target = self._target - subtract_value\n",
    "\n",
    "    def __remove_matched_by_name(self, subtract_value: Set[tuple]) -> None: \n",
    "        \"\"\"Removes matched by name columns from the further processing\n",
    "\n",
    "        Args:\n",
    "            subtract_value (Set[tuple]): Set of matched column\n",
    "        \"\"\"        \n",
    "        \n",
    "        _remove = lambda input_value, subtract_value: [\n",
    "            (x, y) for (x, y) in input_value \n",
    "            if not x.lower() in ([z[0].lower() for z in subtract_value])\n",
    "        ]\n",
    "\n",
    "        self._source = _remove(self._source, subtract_value)\n",
    "        self._target = _remove(self._target, subtract_value)\n",
    "        \n",
    "\n",
    "    def __lower_column_names(self, base_value: Set[tuple]) -> Set[tuple]: \n",
    "        \"\"\"Lower-case all column names of the input set\n",
    "\n",
    "        Args:\n",
    "            base_value (Set[tuple]): Input set of columns\n",
    "\n",
    "        Returns:\n",
    "            Set[tuple]: Output set of columns with lower-case column names\n",
    "        \"\"\"        \n",
    "        return set([(x.lower(), y) for (x, y) in base_value])\n",
    "\n",
    "    def __match_by_name_type_excluding_case(self) -> None:\n",
    "        \"\"\"Matches columns in source and target schemas by name and data type \n",
    "        without taking into account column name case\n",
    "        \"\"\"        \n",
    "        \n",
    "        _source_lowered = self.__lower_column_names(self._source)\n",
    "        _target_lowered = self.__lower_column_names(self._target)\n",
    "\n",
    "        result = self.__match_by_name_and_type(_source_lowered, _target_lowered)\n",
    "\n",
    "        # Remove matched values of case 2 from further processing\n",
    "        self.__remove_matched_by_name(result)\n",
    "\n",
    "        self.__populate_not_matched(\n",
    "            result, \n",
    "            \"The column exists in source and target schemas but it's name is case-mismatched\"\n",
    "        )\n",
    "\n",
    "    def __match_by_name_but_not_type(self) -> None: \n",
    "        \"\"\"Matches columns in source and target schemas only by column name\"\"\"        \n",
    "\n",
    "        x = dict(self._source)\n",
    "        y = dict(self._target)\n",
    "        result = [(k, f\"{x[k]} <=> {y[k]}\") for k in x if k in y and x[k] != y[k]]\n",
    "\n",
    "        # Remove matched values of case 3 from further processing\n",
    "        self.__remove_matched_by_name(result)\n",
    "\n",
    "        self.__populate_not_matched(\n",
    "            result, \n",
    "            \"The column exists in source and target schemas but it is not matched by a data type\"\n",
    "        )        \n",
    "\n",
    "    def __process_remaining_non_matched_columns(self) -> None:\n",
    "        \"\"\"Process remaining not matched columns\"\"\"        \n",
    "\n",
    "        self.__populate_not_matched(self._source, \"The column exists only in the source schema\")\n",
    "\n",
    "        self.__populate_not_matched(self._target, \"The column exists only in the target schema\")\n",
    "        \n",
    "        self.__remove_matched_by_name(self._source)\n",
    "        self.__remove_matched_by_name(self._target)\n",
    "\n",
    "\n",
    "    def __populate_matched(self, input_value: Set[tuple]) -> None:\n",
    "        \"\"\"Populate class property 'matched' with a list of fully matched columns\n",
    "\n",
    "        Args:\n",
    "            input_value (Set[tuple]): The set of tuples with a list of column names and data types\n",
    "        \"\"\"  \n",
    "\n",
    "        for match in input_value:\n",
    "            self.matched.append(\n",
    "                MatchedColumn(name=match[0], data_type=match[1])\n",
    "            )\n",
    "\n",
    "    def __populate_not_matched(self, input_value: Set[tuple], reason: str) -> None:\n",
    "        \"\"\"Populate class property 'not_matched' with a list of columns that didn't match for some \n",
    "        reason with included an actual reason\n",
    "\n",
    "        Args:\n",
    "            input_value (Set[tuple]): The set of tuples with a list of column names and data types\n",
    "            reason (str): Reason for not match\n",
    "        \"\"\"        \n",
    "\n",
    "        for match in input_value:\n",
    "            self.not_matched.append(\n",
    "                NotMatchedColumn(name=match[0], data_type=match[1], reason=reason)\n",
    "            )\n",
    "\n",
    "    def compare(self):\n",
    "        \"\"\"Compares the source and target schemas and populates properties 'matched' and 'not_matched'\"\"\"        \n",
    "\n",
    "        # Case 1: find columns that are matched by name and type and remove them from further processing\n",
    "        self.__match_by_name_and_type()\n",
    "\n",
    "        # Case 2: find columns that match mismatched by name due to case: ID <-> Id\n",
    "        self.__match_by_name_type_excluding_case()\n",
    "\n",
    "        # Case 3: Find columns matched by name, but not by data type\n",
    "        self.__match_by_name_but_not_type()\n",
    "\n",
    "        # Case 4: Find columns that exists only in the source or target\n",
    "        self.__process_remaining_non_matched_columns()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NotMatchedColumn(name='a', data_type='LongType', reason=\"The column exists in source and target schemas but it's name is case-mismatched\"),\n",
       " NotMatchedColumn(name='b', data_type='StringType <=> DoubleType', reason='The column exists in source and target schemas but it is not matched by a data type'),\n",
       " NotMatchedColumn(name='q', data_type='StringType', reason='The column exists only in the source schema'),\n",
       " NotMatchedColumn(name='f', data_type='StringType', reason='The column exists only in the target schema')]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmp = SchemaComparer(\n",
    "    source_schema=df1.schema, \n",
    "    target_schema=df2.schema\n",
    ")\n",
    "\n",
    "cmp.compare()\n",
    "\n",
    "\n",
    "cmp.not_matched\n",
    "#cmp._not_matched_by_name_case\n",
    "#cmp._not_matched_by_type\n",
    "\n",
    "#print(cmp._not_matched_exists_only_in_source)\n",
    "#print(cmp._not_matched_exists_only_in_target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('spalah')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "257711548178d2cb2d5540aed74584fc22fa35e89db7e47a5d0185909a4181b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
