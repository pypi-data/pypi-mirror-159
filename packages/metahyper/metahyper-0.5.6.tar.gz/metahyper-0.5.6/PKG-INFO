Metadata-Version: 2.1
Name: metahyper
Version: 0.5.6
Summary: Parallelisation and distribution framework for neural pipeline search algorithms.
Home-page: https://github.com/automl/metahyper
License: MIT
Keywords: Paralellisation,AutoML,Hyperparameter Optimization,Neural Architecture Search,Neural Pipeline Search
Author: Danny Stoll
Author-email: stolld@cs.uni-freiburg.de
Requires-Python: >=3.7.1,<3.11
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Natural Language :: English
Classifier: Operating System :: Unix
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: System :: Distributed Computing
Requires-Dist: PyYAML (>=6.0,<7.0)
Requires-Dist: dill (>=0.3.4,<0.4.0)
Requires-Dist: mkdocs (>=1.3.0,<2.0.0)
Requires-Dist: more-itertools (>=8.12.0,<9.0.0)
Requires-Dist: numpy (>=1.21.1,<2.0.0)
Requires-Dist: types-PyYAML (>=6.0.4,<7.0.0)
Project-URL: Repository, https://github.com/automl/metahyper
Description-Content-Type: text/markdown

# Metahyper

Parallelisation and distribution framework for neural pipeline search algorithms.

Features:

- Asynchronous parallelization and distribution
- Fault tolerance for crashes and job time limits
- Agnostic to search space, objective functions, ...

![Python versions](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9%20%7C%203.10-informational)
[![License](https://img.shields.io/badge/license-MIT-informational)](LICENSE)

## Installation

Using pip

```bash
pip install metahyper
```

## Usage

Please see our examples in [metahyper_examples](metahyper_examples).

## Contributing

Please see our guidelines and guides for contributors at [CONTRIBUTING.md](docs/CONTRIBUTING.md).

