from __future__ import annotations

import pickle
import tarfile
from pathlib import Path
from typing import TYPE_CHECKING, List

import numpy as np
import pandas as pd
from cropharvest.utils import memoized

from openmapflow.config import PROJECT_ROOT
from openmapflow.config import DataPaths as dp
from openmapflow.data_instance import DataInstance
from openmapflow.utils import try_txt_read

if TYPE_CHECKING:
    from openmapflow.labeled_dataset import LabeledDataset


def create_feature(
    feature_path: str,
    tif_values: np.ndarray,
    tif_lat: float,
    tif_lon: float,
    tif_file: str,
):
    instance = DataInstance(
        labelled_array=tif_values,
        instance_lat=tif_lat,
        instance_lon=tif_lon,
        source_file=tif_file,
    )
    save_path = Path(feature_path)
    save_path.parent.mkdir(exist_ok=True)
    with save_path.open("wb") as f:
        pickle.dump(instance, f)


@memoized
def load_feature(p) -> DataInstance:
    with Path(p).open("rb") as f:
        return pickle.load(f)


@memoized
def load_all_features_as_df() -> pd.DataFrame:
    duplicates_data = try_txt_read(PROJECT_ROOT / dp.DUPLICATES)
    features = []
    files = list((PROJECT_ROOT / dp.FEATURES).glob("*.pkl"))
    print("------------------------------")
    print("Loading all features...")
    non_duplicated_files = []
    for p in files:
        if p.stem not in duplicates_data:
            non_duplicated_files.append(p)
            with p.open("rb") as f:
                features.append(pickle.load(f))
    df = pd.DataFrame([feat.__dict__ for feat in features])
    df["filename"] = non_duplicated_files
    return df


def check_features_df_empty(df: pd.DataFrame) -> str:
    """
    Some exported tif data may have nan values
    """
    if len(df) == 0:
        return "No features found"
    empties = df[df["labelled_array"].isnull()]
    num_empty = len(empties)
    if num_empty > 0:
        return f"\u2716 Found {num_empty} empty features"
    else:
        return "\u2714 Found no empty features"


def check_features_df_duplicates(df: pd.DataFrame) -> str:
    """
    Duplicates can occur when not all tifs have been downloaded
    and different labels are matched to same tif
    """
    if len(df) == 0:
        return "No features found"
    cols_to_check = ["instance_lon", "instance_lat", "source_file"]
    duplicates = df[df.duplicated(subset=cols_to_check)]
    num_dupes = len(duplicates)
    if num_dupes > 0:
        remove_ignore_dupes = input(f"Found {num_dupes} duplicates, remove? [y]/n: ")
        if remove_ignore_dupes.lower() == "n":
            return f"\u2716 Found {num_dupes} duplicates"

        duplicates_data = try_txt_read(PROJECT_ROOT / dp.DUPLICATES)
        feature_filenames = duplicates.filename.apply(lambda p: Path(p).stem).tolist()
        with (PROJECT_ROOT / dp.DUPLICATES).open("w") as f:
            f.write("\n".join(duplicates_data + feature_filenames))
    return "\u2714 No duplicates found"


def create_features(datasets: List[LabeledDataset]):
    report = "DATASET REPORT (autogenerated, do not edit directly)"
    for d in datasets:
        text = d.create_features()
        report += "\n\n" + text

    df = load_all_features_as_df()
    empty_text = check_features_df_empty(df)
    duplicates_text = check_features_df_duplicates(df)
    print(empty_text)
    print(duplicates_text)
    report += "\n\nAll data:\n" + empty_text + "\n" + duplicates_text

    with (PROJECT_ROOT / dp.DATASETS).open("w") as f:
        f.write(report)

    # Compress features for faster CI/CD
    print("Compressing features...")
    with tarfile.open(PROJECT_ROOT / dp.COMPRESSED_FEATURES, "w:gz") as tar:
        tar.add(PROJECT_ROOT / dp.FEATURES, arcname=Path(dp.FEATURES).name)
