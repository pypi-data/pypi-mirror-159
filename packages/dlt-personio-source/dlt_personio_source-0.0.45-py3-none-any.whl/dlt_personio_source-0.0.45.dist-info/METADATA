Metadata-Version: 2.1
Name: dlt-personio-source
Version: 0.0.45
Summary: 
License: apache
Author: Adrian Brudaru
Author-email: adrian@scalevector.ai
Requires-Python: >=3.8,<4.0
Classifier: License :: Other/Proprietary License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Requires-Dist: personio-py (>=0.2.1,<0.3.0)
Requires-Dist: python-dlt
Description-Content-Type: text/markdown

# dlt-personio-source


# Parent tables 
```
'employees', 
'absences', 
'absence_types', 
'attendances'
```
some of these tables have sub-tables

to join the parent table to the sub table, use the join `parent.dlt_id = child.parent_dlt_id`

# Usage
install library

```pipx install dlt-personio-source```
if the library cannot be found, ensure you have the required python version as per the `pyproject.toml`file.

Run the source as below to load a sample data set.

Add credentials and remove the `dummy_data` flag to enable loading your data.

First, import the loading method and add your credentials
```
from dlt_personio_source import load_personio_tables

#target credentials
# example for bigquery
creds = {
  "type": "service_account",
  "project_id": "zinc-mantra-353207",
  "private_key_id": "example",
  "private_key": "",
  "client_email": "example@zinc-mantra-353207.iam.gserviceaccount.com",
  "client_id": "100909481823688180493"}
  
# or example for redshift:
# creds = ["redshift", "database_name", "schema_name", "user_name", "host", "password"]

#Personio credentials
#get credentials at this url - replace"test-1" with your org name
#https://test-1.personio.de/configuration/api/credentials/management
client_id = ''
client_secret = ''
```
then, you can use the code below to do a serial load:
```
# remove some tables from this list of you only want some endpoints
tables = ['employees', 'absences', 'absence_types', 'attendances']
load_personio_tables(client_id=client_id,
                     client_secret=client_secret,
                     target_credentials=creds,
                     tables=tables,
                     schema_name='personio_raw',
                     dummy_data=True)

```
or, for parallel load, create airflow tasks for each table like so:
```
tables = ['employees', 'absences', 'absence_types', 'attendances']
for table in tables:
    load_personio_tables(client_id='',
                         client_secret='',
                         target_credentials=creds,
                         tables = [table],
                         schema_name='personio_raw',
                         dummy_data = True)

```

If you want to do your own pipeline or consume the source differently:
```
from dlt_personio_source import PersonioSource, PersonioSourceDummy

prod = PersonioSource(client_id='',
              client_secret='')
              
dummy = PersonioSourceDummy()

sample_data = dummy.tasks() 

for task in tasks:
    print(task['table_name'])
    for row in task['data']
        print(row)

```
